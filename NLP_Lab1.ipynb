{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhvN0klmTuJBnTFac/cG21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TenzinYangzom158/NLTK/blob/main/NLP_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization** : Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph."
      ],
      "metadata": {
        "id": "V5uVFmQx39Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = '''NLP in retail gives firms the chance to engage in \"intelligent social listening,\" which enables\n",
        "them to better understand customer demands and adjust interactions to revolve around them.\n",
        "Natural language processing technology enables a user journey to be individually personalized at\n",
        "each step.'''"
      ],
      "metadata": {
        "id": "7vcVSDj16h49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67x2Czbd2RcU"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tbJnbOk6vuZ",
        "outputId": "0e1cf8be-a504-4a96-ecf4-d25745a25ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Word Tokenization:  Splitting words in a sentence\n"
      ],
      "metadata": {
        "id": "_Zcs2P-77kqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word Tokenization :\")\n",
        "print(word_tokenize(string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cAYinYZ67hv",
        "outputId": "7452acb5-caae-4bbf-a4c4-6d82d0133113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization :\n",
            "['NLP', 'in', 'retail', 'gives', 'firms', 'the', 'chance', 'to', 'engage', 'in', '``', 'intelligent', 'social', 'listening', ',', \"''\", 'which', 'enables', 'them', 'to', 'better', 'understand', 'customer', 'demands', 'and', 'adjust', 'interactions', 'to', 'revolve', 'around', 'them', '.', 'Natural', 'language', 'processing', 'technology', 'enables', 'a', 'user', 'journey', 'to', 'be', 'individually', 'personalized', 'at', 'each', 'step', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `2. Regexp Tokenizer: splits the sentence into words based on regular expression`\n"
      ],
      "metadata": {
        "id": "oUgoDMM17uE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "R7lyqfv67itT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''The shopping experience at physical stores is greatly personalized, with human shopping\n",
        "assistants standing at attention by the customers all the time.'''\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtQ826qe7owS",
        "outputId": "91c00201-1ed1-4e00-e9b5-6fa65267f2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'shopping',\n",
              " 'experience',\n",
              " 'at',\n",
              " 'physical',\n",
              " 'stores',\n",
              " 'is',\n",
              " 'greatly',\n",
              " 'personalized',\n",
              " 'with',\n",
              " 'human',\n",
              " 'shopping',\n",
              " 'assistants',\n",
              " 'standing',\n",
              " 'at',\n",
              " 'attention',\n",
              " 'by',\n",
              " 'the',\n",
              " 'customers',\n",
              " 'all',\n",
              " 'the',\n",
              " 'time']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = RegexpTokenizer('\\$[\\d\\.] | \\S+')\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L5bsymw9UTk",
        "outputId": "b6cc16bf-b28b-4360-d819-9f46c6d709ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' shopping',\n",
              " ' experience',\n",
              " ' at',\n",
              " ' physical',\n",
              " ' stores',\n",
              " ' is',\n",
              " ' greatly',\n",
              " ' personalized,',\n",
              " ' with',\n",
              " ' human',\n",
              " ' shopping',\n",
              " ' standing',\n",
              " ' at',\n",
              " ' attention',\n",
              " ' by',\n",
              " ' the',\n",
              " ' customers',\n",
              " ' all',\n",
              " ' the',\n",
              " ' time.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer('[^e]')\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmEGLSUn9sli",
        "outputId": "d5a2dd13-c83f-4f4c-fac5-2234a21ae5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T',\n",
              " 'h',\n",
              " ' ',\n",
              " 's',\n",
              " 'h',\n",
              " 'o',\n",
              " 'p',\n",
              " 'p',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'x',\n",
              " 'p',\n",
              " 'r',\n",
              " 'i',\n",
              " 'n',\n",
              " 'c',\n",
              " ' ',\n",
              " 'a',\n",
              " 't',\n",
              " ' ',\n",
              " 'p',\n",
              " 'h',\n",
              " 'y',\n",
              " 's',\n",
              " 'i',\n",
              " 'c',\n",
              " 'a',\n",
              " 'l',\n",
              " ' ',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'r',\n",
              " 's',\n",
              " ' ',\n",
              " 'i',\n",
              " 's',\n",
              " ' ',\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 't',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'p',\n",
              " 'r',\n",
              " 's',\n",
              " 'o',\n",
              " 'n',\n",
              " 'a',\n",
              " 'l',\n",
              " 'i',\n",
              " 'z',\n",
              " 'd',\n",
              " ',',\n",
              " ' ',\n",
              " 'w',\n",
              " 'i',\n",
              " 't',\n",
              " 'h',\n",
              " ' ',\n",
              " 'h',\n",
              " 'u',\n",
              " 'm',\n",
              " 'a',\n",
              " 'n',\n",
              " ' ',\n",
              " 's',\n",
              " 'h',\n",
              " 'o',\n",
              " 'p',\n",
              " 'p',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " '\\n',\n",
              " 'a',\n",
              " 's',\n",
              " 's',\n",
              " 'i',\n",
              " 's',\n",
              " 't',\n",
              " 'a',\n",
              " 'n',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 's',\n",
              " 't',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'a',\n",
              " 't',\n",
              " ' ',\n",
              " 'a',\n",
              " 't',\n",
              " 't',\n",
              " 'n',\n",
              " 't',\n",
              " 'i',\n",
              " 'o',\n",
              " 'n',\n",
              " ' ',\n",
              " 'b',\n",
              " 'y',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " ' ',\n",
              " 'c',\n",
              " 'u',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'm',\n",
              " 'r',\n",
              " 's',\n",
              " ' ',\n",
              " 'a',\n",
              " 'l',\n",
              " 'l',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " ' ',\n",
              " 't',\n",
              " 'i',\n",
              " 'm',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Textblob Tokenization : tokenize into sentence and words and the emoji’s are removed from the punctuation."
      ],
      "metadata": {
        "id": "RcLfr5Px_eTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "mVC1H0sZ-WuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbText = ''' @NLP NLP libraries employ the use of narrow categories (called “Corpora”) to segregate products in\n",
        "highly specific detail so that user search results are more accurate. :-) <3 '''\n"
      ],
      "metadata": {
        "id": "XfCcRgsPAC1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a textblob object\n",
        "blob_object = TextBlob(tbText)"
      ],
      "metadata": {
        "id": "WtNkvlfiAcQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenzie paragraph into words\n",
        "print(\" Word Tokenize : \\n\", blob_object.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs4JJ24UAsyX",
        "outputId": "35326d94-52bf-4b8b-d4f0-de7e41e57ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Word Tokenize : \n",
            " ['NLP', 'NLP', 'libraries', 'employ', 'the', 'use', 'of', 'narrow', 'categories', 'called', '“', 'Corpora', '”', 'to', 'segregate', 'products', 'in', 'highly', 'specific', 'detail', 'so', 'that', 'user', 'search', 'results', 'are', 'more', 'accurate', '3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`4. Sentence Tokenizer `"
      ],
      "metadata": {
        "id": "h7PGs35XCUpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "GDQXIBM_A4Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '''The shopping experience at physical stores is greatly personalized, with human shopping\n",
        "assistants standing at attention by the customers all the time.'''\n",
        "print(sent_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RbZgp5oDaIJ",
        "outputId": "19c9c9ad-6611-4583-8bb5-0031eb3f5097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The shopping experience at physical stores is greatly personalized, with human shopping\\nassistants standing at attention by the customers all the time.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Punctuation-based tokenizer"
      ],
      "metadata": {
        "id": "uERfj7UPDut6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "slHVssAIDi7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuCKijnPEFUW",
        "outputId": "602cd487-20ee-4553-bed5-e29dc5d0e6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'shopping', 'experience', 'at', 'physical', 'stores', 'is', 'greatly', 'personalized', ',', 'with', 'human', 'shopping', 'assistants', 'standing', 'at', 'attention', 'by', 'the', 'customers', 'all', 'the', 'time', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. TreeBank Word Tokenizer: This tokenizer incorporates a variety of common rules for english word tokenization. It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions. \n",
        "\n",
        "For example “don’t” is tokenized as [“do”, “n’t”]"
      ],
      "metadata": {
        "id": "oMZIQSBNENVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "iRnkjJ8sEJ9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''' A consumer's buying experience can be tailored by an e-commerce site, reducing client retention\n",
        "and loyalty to a single factor. '''\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Hs9Yz2FLO4",
        "outputId": "07badfcd-f9e2-40e8-8d38-89960a51c826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'consumer', \"'s\", 'buying', 'experience', 'can', 'be', 'tailored', 'by', 'an', 'e-commerce', 'site', ',', 'reducing', 'client', 'retention', 'and', 'loyalty', 'to', 'a', 'single', 'factor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Tweet Tokenizer: NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "yJJQvnd5FvDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "4ulG8Vi9Fp2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"NLP in Retail Industry 👩‍💻 \"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdsoYDSWGC3W",
        "outputId": "a47ca9ca-57ef-4d79-e4fe-c099d74d7a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'in', 'Retail', 'Industry', '👩\\u200d💻']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. MWET Tokenizer : NLTK’s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens."
      ],
      "metadata": {
        "id": "1q1YtOZxGx51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer"
      ],
      "metadata": {
        "id": "un4Buq5MGgZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''' A consumer's buying experience can be tailored by an e-commerce site, reducing client retention\n",
        "and loyalty to a single factor. '''\n",
        "tokenizer = MWETokenizer()\n",
        "print(tokenizer.tokenize(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbVot2eHRfH",
        "outputId": "0d590adf-542e-4514-a8f8-25557b085fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'consumer', \"'s\", 'buying', 'experience', 'can', 'be', 'tailored', 'by', 'an', 'e-commerce', 'site', ',', 'reducing', 'client', 'retention', 'and', 'loyalty', 'to', 'a', 'single', 'factor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''' A consumer's buying experience can be tailored by an e-commerce site, reducing client retention\n",
        "and loyalty to a single factor. '''\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('buying', 'experience'))\n",
        "print(tokenizer.tokenize(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWvEJv_ZHj9R",
        "outputId": "9e1a8dc4-c758-4a89-e632-7ed0be02a679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'consumer', \"'s\", 'buying_experience', 'can', 'be', 'tailored', 'by', 'an', 'e-commerce', 'site', ',', 'reducing', 'client', 'retention', 'and', 'loyalty', 'to', 'a', 'single', 'factor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. spaCy Tokenizer"
      ],
      "metadata": {
        "id": "YpsHM2ghIL6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "id": "3UuusxSIICYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cadef1-9b56-46e5-babf-2587d87170d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp('''A consumer's buying experience can be tailored by an e-commerce site, reducing client retention\n",
        "and loyalty to a single factor.''')\n",
        "for token in doc:\n",
        "  print(token, token.idx)\n",
        "\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.label_)\n",
        "# print([token for token in doc if token.ent_type_ == 'PERSON'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DltMkExuIbLO",
        "outputId": "ea24ed0f-9e8f-4b4f-aff4-02ea7be926bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A 0\n",
            "consumer 2\n",
            "'s 10\n",
            "buying 13\n",
            "experience 20\n",
            "can 31\n",
            "be 35\n",
            "tailored 38\n",
            "by 47\n",
            "an 50\n",
            "e 53\n",
            "- 54\n",
            "commerce 55\n",
            "site 64\n",
            ", 68\n",
            "reducing 70\n",
            "client 79\n",
            "retention 86\n",
            "\n",
            " 95\n",
            "and 96\n",
            "loyalty 100\n",
            "to 108\n",
            "a 111\n",
            "single 113\n",
            "factor 120\n",
            ". 126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SxCwjAx5I-ED"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}