{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TenzinYangzom158/NLTK/blob/main/ML_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 6: Write a program for stemming Non-English words"
      ],
      "metadata": {
        "id": "nU4M-zs24I0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKwKCDo1C2xw",
        "outputId": "b938420b-9e05-48a9-c7e8-4a337ece08cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"rslp\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, ISRIStemmer, RSLPStemmer, LancasterStemmer\n",
        "import string, re\n",
        "ls =LancasterStemmer()\n",
        "rslp = RSLPStemmer()\n",
        "isris = ISRIStemmer()\n",
        "sb = SnowballStemmer(language=\"russian\")\n",
        "ps = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIzIFUlYC2x0",
        "outputId": "6cf55348-1202-44e0-f079-74161a37ee3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Olivia Newton-John, beloved actress and singer best known for her role as Sandy Olsson in Grease and for a long string of hits topped by 1981’s “Physical,” died on Monday (Aug. 8). She was 73.\n",
            "\n",
            "Her official Facebook page confirmed the news, noting: “Dame Olivia Newton-John (73) passed away peacefully at her Ranch in Southern California this morning, surrounded by family and friends. We ask that everyone please respect the family’s privacy during this very difficult time.”\n",
            "The post continued, “Olivia has been a symbol of triumphs and hope for over 30 years sharing her journey with breast cancer. Her healing inspiration and pioneering experience with plant medicine continues with the Olivia Newton-John Foundation Fund, dedicated to researching plant medicine and cancer. In lieu of flowers, the family asks that any donations be made in her memory to the Olivia Newton-John Foundation Fund (ONJFoundationFund.org).”\n",
            "Newton-John was one of the most popular recording artists of the 1970s and 1980s. She was also a notably nimble artist – moving from early country music hits like “Let Me Be There” to mainstream pop hits like “Have You Never Been Mellow” to sexier, edgier fare like “Make a Move on Me.” Happy\n",
            "olivia\n",
            "belov\n",
            "actress\n",
            "and\n",
            "singer\n",
            "best\n",
            "known\n",
            "for\n",
            "her\n",
            "role\n",
            "as\n",
            "sandi\n",
            "olsson\n",
            "in\n",
            "greas\n",
            "and\n",
            "for\n",
            "a\n",
            "long\n",
            "string\n",
            "of\n",
            "hit\n",
            "top\n",
            "by\n",
            "s\n",
            "physic\n",
            "die\n",
            "on\n",
            "monday\n",
            "she\n",
            "wa\n",
            "her\n",
            "offici\n",
            "facebook\n",
            "page\n",
            "confirm\n",
            "the\n",
            "news\n",
            "note\n",
            "dame\n",
            "olivia\n",
            "pass\n",
            "away\n",
            "peac\n",
            "at\n",
            "her\n",
            "ranch\n",
            "in\n",
            "southern\n",
            "california\n",
            "thi\n",
            "morn\n",
            "surround\n",
            "by\n",
            "famili\n",
            "and\n",
            "friend\n",
            "we\n",
            "ask\n",
            "that\n",
            "everyon\n",
            "pleas\n",
            "respect\n",
            "the\n",
            "famili\n",
            "s\n",
            "privaci\n",
            "dure\n",
            "thi\n",
            "veri\n",
            "difficult\n",
            "the\n",
            "post\n",
            "continu\n",
            "olivia\n",
            "ha\n",
            "been\n",
            "a\n",
            "symbol\n",
            "of\n",
            "triumph\n",
            "and\n",
            "hope\n",
            "for\n",
            "over\n",
            "year\n",
            "share\n",
            "her\n",
            "journey\n",
            "with\n",
            "breast\n",
            "cancer\n",
            "her\n",
            "heal\n",
            "inspir\n",
            "and\n",
            "pioneer\n",
            "experi\n",
            "with\n",
            "plant\n",
            "medicin\n",
            "continu\n",
            "with\n",
            "the\n",
            "olivia\n",
            "foundat\n",
            "fund\n",
            "dedic\n",
            "to\n",
            "research\n",
            "plant\n",
            "medicin\n",
            "and\n",
            "cancer\n",
            "in\n",
            "lieu\n",
            "of\n",
            "flower\n",
            "the\n",
            "famili\n",
            "ask\n",
            "that\n",
            "ani\n",
            "donat\n",
            "be\n",
            "made\n",
            "in\n",
            "her\n",
            "memori\n",
            "to\n",
            "the\n",
            "olivia\n",
            "foundat\n",
            "fund\n",
            "wa\n",
            "one\n",
            "of\n",
            "the\n",
            "most\n",
            "popular\n",
            "record\n",
            "artist\n",
            "of\n",
            "the\n",
            "and\n",
            "she\n",
            "wa\n",
            "also\n",
            "a\n",
            "notabl\n",
            "nimbl\n",
            "artist\n",
            "move\n",
            "from\n",
            "earli\n",
            "countri\n",
            "music\n",
            "hit\n",
            "like\n",
            "let\n",
            "me\n",
            "be\n",
            "there\n",
            "to\n",
            "mainstream\n",
            "pop\n",
            "hit\n",
            "like\n",
            "have\n",
            "you\n",
            "never\n",
            "been\n",
            "mellow\n",
            "to\n",
            "sexier\n",
            "edgier\n",
            "fare\n",
            "like\n",
            "make\n",
            "a\n",
            "move\n",
            "on\n",
            "happi\n"
          ]
        }
      ],
      "source": [
        "samp =open(\"blog.txt\", encoding=\"utf-8\")\n",
        "txt = samp.read()\n",
        "print(txt)\n",
        "\n",
        "tok = word_tokenize(txt)\n",
        "\n",
        "tok = [i for i in tok if not i in string.punctuation and stopwords.words(\"english\") and re.search(\"^[a-zA-Z]+$\",i)]\n",
        "\n",
        "for i in tok:\n",
        "    print(ps.stem(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOsYmx7PC2x3",
        "outputId": "b6a492d9-1d0b-4c1c-991d-16c07beef328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мадхья-прадеш : мадхья-прадеш\n",
            "зарегистрировал : зарегистрирова\n",
            "шесть : шест\n",
            "новых : нов\n",
            "случаев : случа\n",
            "коронавируса : коронавирус\n",
            "за : за\n",
            "последние : последн\n",
            "24 : 24\n",
            "часа : час\n",
            "в : в\n",
            "результате : результат\n",
            "чего : чег\n",
            "число : числ\n",
            "инфекций : инфекц\n",
            "достигло : достигл\n",
            "10 : 10\n",
            "41 : 41\n",
            "266 : 266\n",
            "сообщил : сообщ\n",
            "в : в\n",
            "среду : сред\n",
            "чиновник : чиновник\n",
            "из : из\n",
            "департамента : департамент\n",
            "здравоохранения : здравоохранен\n",
            "штата : штат\n",
            "целых : цел\n",
            "10 : 10\n",
            "пациентов : пациент\n",
            "были : был\n",
            "выписаны : выписа\n",
            "из : из\n",
            "больниц : больниц\n",
            "в : в\n",
            "течение : течен\n",
            "дня : дня\n",
            "увеличив : увелич\n",
            "количество : количеств\n",
            "выздоровлений : выздоровлен\n",
            "до : до\n",
            "10 : 10\n",
            "30 : 30\n",
            "487 : 487\n",
            "в : в\n",
            "то : то\n",
            "время : врем\n",
            "как : как\n",
            "число : числ\n",
            "жертв : жертв\n",
            "составило : состав\n",
            "10 : 10\n",
            "734 : 734\n",
            "поскольку : поскольк\n",
            "не : не\n",
            "сообщалось : сообща\n",
            "о : о\n",
            "новых : нов\n",
            "жертвах : жертв\n",
            "сказал : сказа\n",
            "чиновник : чиновник\n",
            "несмотря : несмотр\n",
            "на : на\n",
            "всплеск : всплеск\n",
            "случаев : случа\n",
            "covid : covid\n",
            "в : в\n",
            "дели : дел\n",
            "госпитализация : госпитализац\n",
            "до : до\n",
            "сих : сих\n",
            "пор : пор\n",
            "была : был\n",
            "низкой : низк\n",
            "составляя : составл\n",
            "менее : мен\n",
            "трех : трех\n",
            "процентов : процент\n",
            "от : от\n",
            "общего : общ\n",
            "числа : числ\n",
            "активных : активн\n",
            "случаев : случа\n",
            "согласно : согласн\n",
            "правительственным : правительствен\n",
            "данным : дан\n",
            "совокупное : совокупн\n",
            "количество : количеств\n",
            "доз : доз\n",
            "вакцины : вакцин\n",
            "против : прот\n",
            "covid-19 : covid-19\n",
            "введенных : введен\n",
            "в : в\n",
            "стране : стран\n",
            "в : в\n",
            "среду : сред\n",
            "превысило : превыс\n",
            "187 : 187\n",
            "крор : крор\n",
            "по : по\n",
            "данным : дан\n",
            "министерства : министерств\n",
            "здравоохранения : здравоохранен\n",
            "союза : союз\n"
          ]
        }
      ],
      "source": [
        "samp1 = open(\"russian.txt\", encoding=\"utf-8\")\n",
        "txt1 = samp1.read()\n",
        "\n",
        "tok1 = word_tokenize(txt1)\n",
        "\n",
        "tok1 = [i.lower() for i in tok1 if not i in string.punctuation and stopwords.words(\"russian\")]\n",
        "\n",
        "for i in tok1:\n",
        "    print(f\"{i} : {sb.stem(i)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0IonrhxC2x5",
        "outputId": "b6792870-3afa-4996-ab60-a19f36028fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "توفيت : وفت\n",
            "أوليفيا : ولف\n",
            "نيوتن : نيو\n",
            "جون، : جون،\n",
            "الممثلة : مثل\n",
            "والمغنية : غنة\n",
            "المحبوبة : حبب\n",
            "التي : التي\n",
            "اشتهرت : شهر\n",
            "بدورها : بدر\n",
            "ساندي : سند\n",
            "أولسون : ولس\n",
            "في : في\n",
            "فيلم : يلم\n",
            "grease : grease\n",
            "وسلسلة : لسل\n",
            "طويلة : طول\n",
            "من : من\n",
            "الأغاني : اغا\n",
            "الناجحة : نجح\n",
            "التي : التي\n",
            "تصدرها : صدر\n",
            "فيلم : يلم\n",
            "عام : عام\n",
            "1981، : 1981،\n",
            "يوم : يوم\n",
            "الاثنين : اثن\n",
            "8 : 8\n",
            "أغسطس : غسطس\n",
            "كانت : كانت\n",
            "تبلغ : بلغ\n",
            "من : من\n",
            "العمر : عمر\n",
            "73 : 73\n",
            "عاما : عما\n",
            "وأكدت : أكد\n",
            "صفحتها : صفح\n",
            "الرسمية : رسم\n",
            "على : على\n",
            "فيسبوك : سبو\n",
            "الخبر، : خبر،\n",
            "مشيرة : شير\n",
            "إلى : الى\n",
            "أن : ان\n",
            "`` : ``\n",
            "السيدة : سيد\n",
            "أوليفيا : ولف\n",
            "نيوتن-جون : ن-ج\n",
            "73 : 73\n",
            "عاما : عما\n",
            "توفيت : وفت\n",
            "بسلام : سلم\n",
            "في : في\n",
            "مزرعتها : زرع\n",
            "في : في\n",
            "جنوب : جنب\n",
            "كاليفورنيا : يفر\n",
            "صباح : صبح\n",
            "اليوم، : وم،\n",
            "محاطة : حاط\n",
            "بالعائلة : عئل\n",
            "والأصدقاء : صدقاء\n",
            "نطلب : طلب\n",
            "من : من\n",
            "الجميع : جمع\n",
            "احترام : حرم\n",
            "خصوصية : خصص\n",
            "العائلة : عئل\n",
            "خلال : خلل\n",
            "هذا : هذا\n",
            "الوقت : وقت\n",
            "العصيب : عصب\n",
            "للغاية : غية\n",
            "'' : ''\n"
          ]
        }
      ],
      "source": [
        "samp2 = open(\"arabic.txt\", encoding=\"utf-8\")\n",
        "txt2 = samp2.read()\n",
        "\n",
        "tok2 = word_tokenize(txt2)\n",
        "\n",
        "tok2 = [i.lower() for i in tok2 if not i in string.punctuation and stopwords.words(\"arabic\")]\n",
        "\n",
        "for i in tok2:\n",
        "    print(f\"{i} : {isris.stem(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skXLtxtUC2x7",
        "outputId": "8ccc7aa9-24f8-4c01-94d6-c552391251e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "amada : am\n",
            "atriz : atriz\n",
            "e : e\n",
            "cantora : can\n",
            "mais : mais\n",
            "conhecida : conhec\n",
            "pelo : pel\n",
            "seu : seu\n",
            "papel : papel\n",
            "como : com\n",
            "sandy : sandy\n",
            "olsson : olsson\n",
            "em : em\n",
            "grease : greas\n",
            "e : e\n",
            "por : por\n",
            "uma : uma\n",
            "longa : long\n",
            "série : séri\n",
            "de : de\n",
            "sucessos : sucess\n",
            "superados : super\n",
            "por : por\n",
            "`` : ``\n",
            "physical : physic\n",
            "'' : ''\n",
            "de : de\n",
            "1981 : 1981\n",
            "morreu : morr\n",
            "esta : est\n",
            "segunda-feira : segunda-f\n",
            "8 : 8\n",
            "de : de\n",
            "agosto : agost\n",
            "tinha : tinh\n",
            "73 : 73\n",
            "anos : ano\n",
            "a : a\n",
            "sua : sua\n",
            "página : págin\n",
            "oficial : ofic\n",
            "no : no\n",
            "facebook : facebook\n",
            "confirmou : confirm\n",
            "a : a\n",
            "notícia : notíc\n",
            "referindo : refer\n",
            "`` : ``\n",
            "a : a\n",
            "dama : dam\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "73 : 73\n",
            "faleceu : falec\n",
            "pacificamente : pacific\n",
            "no : no\n",
            "seu : seu\n",
            "rancho : ranch\n",
            "no : no\n",
            "sul : sul\n",
            "da : da\n",
            "califórnia : califórn\n",
            "esta : est\n",
            "manhã : manhã\n",
            "rodeada : rode\n",
            "por : por\n",
            "familiares : famili\n",
            "e : e\n",
            "amigos : amig\n",
            "pedimos : ped\n",
            "que : que\n",
            "respeitem : respeit\n",
            "a : a\n",
            "privacidade : privac\n",
            "da : da\n",
            "família : famíl\n",
            "neste : nest\n",
            "momento : moment\n",
            "tão : tão\n",
            "difícil : difícil\n",
            "`` : ``\n",
            "a : a\n",
            "publicação : public\n",
            "continuou : continu\n",
            "`` : ``\n",
            "olivia : oliv\n",
            "tem : tem\n",
            "sido : sid\n",
            "um : um\n",
            "símbolo : símbol\n",
            "de : de\n",
            "triunfos : triunf\n",
            "e : e\n",
            "esperança : esperanç\n",
            "há : há\n",
            "mais : mais\n",
            "de : de\n",
            "30 : 30\n",
            "anos : ano\n",
            "partilhando : partilh\n",
            "a : a\n",
            "sua : sua\n",
            "jornada : jorn\n",
            "com : com\n",
            "cancro : cancr\n",
            "da : da\n",
            "mama : mam\n",
            "a : a\n",
            "sua : sua\n",
            "inspiração : inspir\n",
            "curativa : cura\n",
            "e : e\n",
            "experiência : experi\n",
            "pioneira : pioneir\n",
            "na : na\n",
            "medicina : medicin\n",
            "vegetal : veget\n",
            "continua : continu\n",
            "com : com\n",
            "o : o\n",
            "fundo : fund\n",
            "da : da\n",
            "fundação : fund\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "dedicado : dedic\n",
            "à : à\n",
            "investigação : investig\n",
            "de : de\n",
            "medicina : medicin\n",
            "vegetal : veget\n",
            "e : e\n",
            "cancro : cancr\n",
            "em : em\n",
            "vez : vez\n",
            "de : de\n",
            "flores : fl\n",
            "a : a\n",
            "família : famíl\n",
            "pede : ped\n",
            "que : que\n",
            "quaisquer : quaisqu\n",
            "doações : doaç\n",
            "sejam : sej\n",
            "feitas : feit\n",
            "em : em\n",
            "sua : sua\n",
            "memória : memór\n",
            "para : par\n",
            "o : o\n",
            "fundo : fund\n",
            "da : da\n",
            "fundação : fund\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "onjfoundationfund.org : onjfoundationfund.org\n",
            "'' : ''\n",
            "newton-john : newton-john\n",
            "foi : foi\n",
            "um : um\n",
            "dos : do\n",
            "artistas : artist\n",
            "de : de\n",
            "gravação : grav\n",
            "mais : mais\n",
            "populares : popul\n",
            "das : da\n",
            "décadas : déc\n",
            "de : de\n",
            "1970 : 1970\n",
            "e : e\n",
            "1980 : 1980\n",
            "foi : foi\n",
            "também : também\n",
            "uma : uma\n",
            "artista : artist\n",
            "notavelmente : notavel\n",
            "ágil : ágil\n",
            "– : –\n",
            "passando : pass\n",
            "de : de\n",
            "sucessos : sucess\n",
            "musicais : music\n",
            "country : country\n",
            "como : com\n",
            "`` : ``\n",
            "let : let\n",
            "me : me\n",
            "be : be\n",
            "there : ther\n",
            "'' : ''\n",
            "para : par\n",
            "sucessos : sucess\n",
            "pop : pop\n",
            "mainstream : mainstre\n",
            "como : com\n",
            "`` : ``\n",
            "have : hav\n",
            "you : you\n",
            "never : nev\n",
            "been : been\n",
            "mellow : mellow\n",
            "'' : ''\n",
            "para : par\n",
            "sexier : sexi\n",
            "edgier : edgi\n",
            "fare : far\n",
            "como : com\n",
            "`` : ``\n",
            "make : mak\n",
            "a : a\n",
            "move : mov\n",
            "on : on\n",
            "me : me\n",
            "'' : ''\n"
          ]
        }
      ],
      "source": [
        "samp3 = open(\"portu.txt\", encoding=\"utf-8\")\n",
        "txt3 = samp3.read()\n",
        "\n",
        "tok3 = word_tokenize(txt3)\n",
        "\n",
        "tok3 = [i.lower() for i in tok3 if not i in string.punctuation and stopwords.words(\"portuguese\")]\n",
        "\n",
        "for i in tok3:\n",
        "    print(f\"{i} : {rslp.stem(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKZnMOPwC2x8",
        "outputId": "acbd2654-8e1a-42e6-fd58-105f8f2301c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "beloved : belov\n",
            "actress : actress\n",
            "and : and\n",
            "singer : sing\n",
            "best : best\n",
            "known : known\n",
            "for : for\n",
            "her : her\n",
            "role : rol\n",
            "as : as\n",
            "sandy : sandy\n",
            "olsson : olsson\n",
            "in : in\n",
            "grease : greas\n",
            "and : and\n",
            "for : for\n",
            "a : a\n",
            "long : long\n",
            "string : string\n",
            "of : of\n",
            "hits : hit\n",
            "topped : top\n",
            "by : by\n",
            "1981 : 1981\n",
            "’ : ’\n",
            "s : s\n",
            "“ : “\n",
            "physical : phys\n",
            "” : ”\n",
            "died : died\n",
            "on : on\n",
            "monday : monday\n",
            "aug. : aug.\n",
            "8 : 8\n",
            "she : she\n",
            "was : was\n",
            "73 : 73\n",
            "her : her\n",
            "official : off\n",
            "facebook : facebook\n",
            "page : pag\n",
            "confirmed : confirm\n",
            "the : the\n",
            "news : new\n",
            "noting : not\n",
            "“ : “\n",
            "dame : dam\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "73 : 73\n",
            "passed : pass\n",
            "away : away\n",
            "peacefully : peac\n",
            "at : at\n",
            "her : her\n",
            "ranch : ranch\n",
            "in : in\n",
            "southern : southern\n",
            "california : californ\n",
            "this : thi\n",
            "morning : morn\n",
            "surrounded : surround\n",
            "by : by\n",
            "family : famy\n",
            "and : and\n",
            "friends : friend\n",
            "we : we\n",
            "ask : ask\n",
            "that : that\n",
            "everyone : everyon\n",
            "please : pleas\n",
            "respect : respect\n",
            "the : the\n",
            "family : famy\n",
            "’ : ’\n",
            "s : s\n",
            "privacy : priv\n",
            "during : dur\n",
            "this : thi\n",
            "very : very\n",
            "difficult : difficult\n",
            "time. : time.\n",
            "” : ”\n",
            "the : the\n",
            "post : post\n",
            "continued : continu\n",
            "“ : “\n",
            "olivia : oliv\n",
            "has : has\n",
            "been : been\n",
            "a : a\n",
            "symbol : symbol\n",
            "of : of\n",
            "triumphs : triumph\n",
            "and : and\n",
            "hope : hop\n",
            "for : for\n",
            "over : ov\n",
            "30 : 30\n",
            "years : year\n",
            "sharing : shar\n",
            "her : her\n",
            "journey : journey\n",
            "with : with\n",
            "breast : breast\n",
            "cancer : cant\n",
            "her : her\n",
            "healing : heal\n",
            "inspiration : inspir\n",
            "and : and\n",
            "pioneering : pion\n",
            "experience : expery\n",
            "with : with\n",
            "plant : plant\n",
            "medicine : medicin\n",
            "continues : continu\n",
            "with : with\n",
            "the : the\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "foundation : found\n",
            "fund : fund\n",
            "dedicated : ded\n",
            "to : to\n",
            "researching : research\n",
            "plant : plant\n",
            "medicine : medicin\n",
            "and : and\n",
            "cancer : cant\n",
            "in : in\n",
            "lieu : lieu\n",
            "of : of\n",
            "flowers : flow\n",
            "the : the\n",
            "family : famy\n",
            "asks : ask\n",
            "that : that\n",
            "any : any\n",
            "donations : don\n",
            "be : be\n",
            "made : mad\n",
            "in : in\n",
            "her : her\n",
            "memory : mem\n",
            "to : to\n",
            "the : the\n",
            "olivia : oliv\n",
            "newton-john : newton-john\n",
            "foundation : found\n",
            "fund : fund\n",
            "onjfoundationfund.org : onjfoundationfund.org\n",
            "” : ”\n",
            "newton-john : newton-john\n",
            "was : was\n",
            "one : on\n",
            "of : of\n",
            "the : the\n",
            "most : most\n",
            "popular : popul\n",
            "recording : record\n",
            "artists : art\n",
            "of : of\n",
            "the : the\n",
            "1970s : 1970s\n",
            "and : and\n",
            "1980s : 1980s\n",
            "she : she\n",
            "was : was\n",
            "also : also\n",
            "a : a\n",
            "notably : not\n",
            "nimble : nimbl\n",
            "artist : art\n",
            "– : –\n",
            "moving : mov\n",
            "from : from\n",
            "early : ear\n",
            "country : country\n",
            "music : mus\n",
            "hits : hit\n",
            "like : lik\n",
            "“ : “\n",
            "let : let\n",
            "me : me\n",
            "be : be\n",
            "there : ther\n",
            "” : ”\n",
            "to : to\n",
            "mainstream : mainstream\n",
            "pop : pop\n",
            "hits : hit\n",
            "like : lik\n",
            "“ : “\n",
            "have : hav\n",
            "you : you\n",
            "never : nev\n",
            "been : been\n",
            "mellow : mellow\n",
            "” : ”\n",
            "to : to\n",
            "sexier : sexy\n",
            "edgier : edgy\n",
            "fare : far\n",
            "like : lik\n",
            "“ : “\n",
            "make : mak\n",
            "a : a\n",
            "move : mov\n",
            "on : on\n",
            "me. : me.\n",
            "” : ”\n",
            "happy : happy\n"
          ]
        }
      ],
      "source": [
        "samp4 = open(\"blog.txt\", encoding=\"utf-8\")\n",
        "txt4 = samp4.read()\n",
        "\n",
        "tok4 = word_tokenize(txt4)\n",
        "\n",
        "tok4 = [i.lower() for i in tok4 if not i in string.punctuation and stopwords.words(\"english\")]\n",
        "\n",
        "for i in tok4:\n",
        "    print(f\"{i} : {ls.stem(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qx0K7hVMGLif"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    },
    "colab": {
      "name": "ML_Lab6.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}